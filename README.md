# Algoritmi di Ordinamento e di Ricerca

Tutti gli algoritmi di ordinamento condividono l‚Äôobiettivo di generare un array ordinato, ma il modo in cui ciascun algoritmo svolge questa attivit√† pu√≤ variare.
Quando si lavora con qualsiasi tipo di algoritmo, √® importante sapere quanto √® veloce e in quanto spazio opera.
Questi fattori sono indicati come **Time Complexity** e **Space Complexity** dell‚Äôalgoritmo.

Questi sono alcuni algoritmi di ordinamento noti:

* Bubble Sort
* Selection Sort
* Insertion Sort
* Merge Sort
* QuickSort

Quando si sceglie un algoritmo di ordinamento, √® necessario considerare la quantit√† di dati che si sta ordinando e il tempo necessario per implementare l‚Äôalgoritmo.
Ad esempio, QuickSort √® molto efficiente, ma pu√≤ essere piuttosto complicato da implementare, mentre, Bubble Sort √® semplice da implementare, ma √® lento.

Per ordinare piccoli set di dati, Bubble Sort potrebbe essere un‚Äôopzione migliore poich√© pu√≤ essere implementato rapidamente, ma per set di dati pi√π grandi, la velocit√† di QuickSort potrebbe valere la pena di implementare l‚Äôalgoritmo seppur complesso.

***Prima di buttarsi sull‚Äôalgoritmo che preferiamo o che conosciamo meglio vale la pena valutare tutti gli algoritmi per determinare la soluzione migliore.***

## Bubble Sort

Il Bubble Sort √® un algoritmo utilizzato per ordinare una lista di elementi, ad esempio elementi in un array.
L‚Äôalgoritmo confronta due elementi adiacenti e quindi li scambia se non sono in ordine.
Il processo viene ripetuto fino a quando non √® pi√π necessario lo scambio.

Per esempio, consideriamo il seguente array [3,1,5,2]:

1) [1,3,5,2], i primi due elementi vengono confrontati e scambiati.
2) [1,3,5,2], la coppia successiva viene confrontata e non scambiata, poich√© sono in ordine.
3) [1,3,2,5], gli ultimi due elementi vengono scambiati.

Questa √® stata la prima iterazione sull‚Äôarray.

La seconda iterazione, invece, genera questi risultati:

1) [1,3,2,5]
2) [1,2,3,5]
3) [1,2,3,5]

La terza iterazione non cambier√† alcun elemento, il che significa che l‚Äôelenco √® ordinato!

**Il vantaggio principale di Bubble Sort √® la semplicit√† dell‚Äôalgoritmo.
In termini di complessit√†, Bubble Sort √® considerato non ottimale, poich√© ha richiesto pi√π iterazioni sull‚Äôarray.**

Nel peggiore dei casi, in cui tutti gli elementi devono essere scambiati, l'algoritmo richieder√†:

![](images/1.jpg)

scambi (n √® il numero di elementi).

Implementazione: [C++](https://github.com/AntonioBerna/Algoritmi/blob/master/C%2B%2B/1_Bubble%20Sort/Bubble%20Sort.cpp), [Python](https://github.com/AntonioBerna/Algoritmi/blob/master/Python/1_Bubble%20Sort/Bubble%20Sort.py)

**Bonus: L‚Äôalgoritmo si chiama Bubble Sort, perch√© ad ogni iterazione l‚Äôelemento pi√π piccolo dell'array si sposta verso l‚Äôalto, proprio come una bolla sale sulla superficie dell'acqua.**

## Selection Sort

Il Selection Sort √® un semplice algoritmo che trova l‚Äôelemento pi√π piccolo nell‚Äôarray e lo scambia con l‚Äôelemento nella prima posizione, quindi trova il secondo elemento pi√π piccolo e lo scambia con l‚Äôelemento nella seconda posizione, e continua in questo modo fino a quando l‚Äôintero array √® ordinato.

Per esempio, consideriamo il seguente array [3,1,5,2]:

1) L‚Äôelemento pi√π piccolo √® 1. Lo scambiamo con il primo elemento. Risultato: [1,3,5,2]
2) Il secondo pi√π piccolo viene scambiato con il secondo elemento. Risultato: [1,2,5,3]
3) Il terzo pi√π piccolo viene scambiato con il terzo elemento. Risultato: [1,2,3,5]

L‚Äôarray √® ordinato!

Implementazione: [C++](https://github.com/AntonioBerna/Algoritmi/blob/master/C%2B%2B/2_Selection%20Sort/Selection%20Sort.cpp), [Python](https://github.com/AntonioBerna/Algoritmi/blob/master/Python/2_Selection%20Sort/Selection%20Sort.py)

**Bonus: L'algoritmo √® efficiente per gli array di piccole dimensioni, ma molto inefficiente per quelli di grandi dimensioni.**

## Insertion Sort

L'Insertion Sort √® un semplice algoritmo che funziona nello stesso modo in cui vengono ordinate le carte da gioco nelle nostre mani. Ordiniamo le prime due carte e quindi posizioniamo la terza carta nella posizione appropriata all‚Äôinterno delle prime due, quindi la quarta viene posizionata tra le prime tre e cos√¨ via fino a quando l‚Äôintera mano non viene ordinata. Durante un‚Äôiterazione, un elemento dell'array viene inserito nella parte ordinata dell‚Äôarray alla sua sinistra. Quindi, sostanzialmente, per ogni iterazione, abbiamo un array di elementi ordinati a sinistra e un array di elementi ancora da ordinare a destra.

Sembra confuso?

Diamo un‚Äôocchiata ad un esempio per capire meglio l‚Äôalgoritmo. Consideriamo il seguente array [3,1,5,2]:

1) Iniziamo con il secondo elemento (1) e lo posizioniamo correttamente nell‚Äôarray fra i primi due elementi. Risultato: [1,3,5,2], ora abbiamo un array ordinato a sinistra ([1,3]) e gli altri elementi ancora da ordinare a destra.
2) L‚Äôelemento successivo √® 5. Inserendolo nell‚Äôarray a sinistra si ottiene: [1,3,5,2].
3) L‚Äôultimo elemento (2) viene inserito nella posizione corrispondente, risultando in: [1,2,3,5].

Ora l‚Äôarray √® ordinato!

Implementazione: [C++](https://github.com/AntonioBerna/Algoritmi/blob/master/C%2B%2B/3_Insertion%20Sort/Insertion%20Sort.cpp), [Python](https://github.com/AntonioBerna/Algoritmi/blob/master/Python/3_Insertion%20Sort/Insertion%20Sort.py)

**Bonus: L'algoritmo √® efficiente per gli array di piccole dimensioni, ma molto inefficiente per quelli di grandi dimensioni.**

## Merge Sort

Il Merge Sort appartiene alla categoria degli algoritmi **Divide Et Impera**. Questi algoritmi funzionano suddividendo grossi problemi in pi√π piccoli e pi√π facilmente risolvibili. L‚Äôidea dell‚Äôalgoritmo di tipo Merge √® di dividere l‚Äôarray a met√† pi√π e pi√π volte fino a quando ogni singolo array √® lungo solo un elemento. Quindi gli elementi vengono rimessi insieme (uniti) e ordinati.

Diamo un‚Äôocchiata ad un esempio, iniziamo dividendo l‚Äôarray [31, 4, 88, 1, 4, 2, 42]:

1) Divido l‚Äôarray in 2 parti: [31, 4, 88, 1] [4, 2, 42]
2) Divido l‚Äôarray in 4 parti: [31, 4] [88, 1] [4, 2] [42]
3) Divido l‚Äôarray in singoli elementi: [31] [4] [88] [1] [4] [2] [42]

Ora dobbiamo riunirli di nuovo insieme ordinandoli:

1) [4, 31] [1, 88] [2, 4] [42]
2) [1,4,31,88] [2,4,42]
3) [1,2,4,4,31,42,88].

Ora l‚Äôarray √® ordinato!

**L‚Äôidea alla base dell‚Äôalgoritmo √® che le parti pi√π piccole sono pi√π facili da ordinare e l'operazione di unione √® la parte pi√π importante dell‚Äôalgoritmo.**

Implementazione: [C++](https://github.com/AntonioBerna/Algoritmi/blob/master/C%2B%2B/4_Merge%20Sort/Merge%20Sort.cpp), [Python](https://github.com/AntonioBerna/Algoritmi/blob/master/Python/4_Merge%20Sort/Merge%20Sort.py)

**Bonus: Il Merge Sort √® utile per ordinare i Linked Lists (cio√® una Struttura Dati), poich√© le operazioni di unione possono essere implementate senza spazio aggiuntivo.**

## QuickSort

Anche il QuickSort appartiene alla categoria degli algoritmi **Divide Et Impera**, cio√®, suddivide grossi problemi in pi√π piccoli e pi√π facilmente risolvibili. L‚Äôidea alla base del QuickSort √® di scegliere un elemento perno (pivot). Le versioni di QuickSort si differenziano per il metodo di **pivot picking**. Il primo, l‚Äôultimo, il mediano o anche un elemento selezionato casualmente √® un possibile candidato da scegliere come perno. **La parte principale del processo di ordinamento √® il partizionamento.**
Una volta scelto il perno, l‚Äôarray viene suddiviso in due met√†: una met√† contenente tutti gli elementi pi√π piccoli del perno e l‚Äôaltra contenente tutti gli elementi pi√π grandi del perno. Quindi, lo stesso processo si ripete in modo ricorsivo per le restanti met√† dell'array, risultando infine in un array ordinato.

Consideriamo il seguente esempio [2, 0, 7, 4, 3]:

1) Scegliamo 3 (ultimo elemento in posizione 4) come perno.
2) Dopo aver fatto 4 confronti otteniamo le due met√† dell'array pi√π il perno: [2,0] (3) [7,4]
3) Ora, lo stesso processo si ripete per le due met√† dell'array: scegliamo (0) come perno per il sotto l'array con gli elementi pi√π piccoli e (4) per il sotto l'array con gli elementi pi√π grandi.
4) Dopo un confronto per ogni met√†, otteniamo: [(0)[2]] (3) [(4)[7]], che risulta essere un array ordinato!

Implementazione: [C++](https://github.com/AntonioBerna/Algoritmi/blob/master/C%2B%2B/5_QuickSort/QuickSort.cpp), [Python](https://github.com/AntonioBerna/Algoritmi/blob/master/Python/5_QuickSort/QuickSort.py)

### Vantaggi e Svantaggi del QuickSort

***La scelta del pivot (perno) fa una grande differenza, poich√© una selezione del pivot non riuscita pu√≤ ridurre significativamente la velocit√† dell‚Äôalgoritmo. Una variante di QuickSort √® il QuickSort a 3 vie, che lo rende pi√π conveniente per i dati con elevata ridondanza. Invece, la versione casuale del QuickSort √® la pi√π efficiente: si sceglie il perno in modo casuale, evitando cos√¨ i casi peggiori per modelli particolari (come array gi√† ordinati), sebbene non del tutto.***

## Linear Search

La Ricerca Lineare √® un algoritmo di ricerca molto semplice. Ogni elemento viene controllato e se viene trovata una corrispondenza, viene restituito quel particolare elemento, altrimenti la ricerca continua fino alla fine dell'elenco. La Ricerca Lineare non richiede un elenco ordinato.

Proviamo a cercare di trovare il valore x nell'elenco. L‚Äôalgoritmo √® formato dai seguenti passaggi:

1) Inizia dall'elemento pi√π a sinistra dell'elenco e uno per uno confronta x con ciascun elemento dell'array.
2) Se x corrisponde a un elemento, restituisce l'indice.
3) Se x non corrisponde a nessuno degli elementi, restituisce -1.

Implementazione: [C++](https://github.com/AntonioBerna/Algoritmi/blob/master/C%2B%2B/6_Linear%20Search/LinearSearch.cpp), [Python](https://github.com/AntonioBerna/Algoritmi/blob/master/Python/6_Linear%20Search/LinearSearch.py)

**Bonus: La Ricerca Lineare viene raramente utilizzata perch√© altri algoritmi di ricerca, come l'algoritmo di Ricerca Binaria, consentono confronti di ricerca significativamente pi√π veloci.**

## Binary Search

La Ricerca Binaria √® un algoritmo di ricerca rapida che funziona su elenchi ordinati. Appartiene alla categoria **Divide Et Impera**, il che significa che scompone grossi problemi in problemi pi√π piccoli e pi√π facilmente risolvibili. L‚Äôalgoritmo cerca una corrispondenza confrontando l‚Äôelemento centrale di un array. Se si verifica una corrispondenza, viene restituito l‚Äôindice dell‚Äôelemento. Se l‚Äôelemento centrale √® maggiore dell‚Äôelemento da trovare, viene confrontato l‚Äôelemento centrale del sotto array a sinistra, altrimenti viene confrontato l‚Äôelemento centrale del sotto array a destra. Questo processo si ripete sui sotto array fino a quando la dimensione del sotto array non si riduce a zero. Fondamentalmente, l‚Äôarray √® diviso in due met√† e la ricerca continua nella met√† in cui l‚Äôelemento ha la possibilit√† di essere localizzato. Questo √® il motivo per cui l‚Äôalgoritmo richiede un elenco ordinato.

Prendiamo un array ordinato di esempio [2,5,16,18,24,42] e cerchiamo l‚Äôelemento 24:

1) Prendiamo l‚Äôelemento pi√π centrale (18) e lo confrontiamo con 24. Avremmo potuto prendere anche un altro elemento centrale ma di solito si divide per 2 il numero di elementi nell‚Äôarray e si prende come risultato l‚Äôindice dell‚Äôelemento centrale.
2) 18 < 24, quindi dividiamo l‚Äôarray in due parti e continuiamo a lavorare con il sotto array a destra: [24, 42].
3) Lo stesso processo si ripete e visto che 42 > 24, consideriamo il sotto array sinistro: [24].

Elemento trovato!

Proviamo a cercare l‚Äôelemento 4 nell'array in figura, prendendo come elemento centrale 7:

![](images/2.png)

Implementazione: [C++](https://github.com/AntonioBerna/Algoritmi/blob/master/C%2B%2B/7_Binary%20Search/BinarySearch.cpp), [Python](https://github.com/AntonioBerna/Algoritmi/blob/master/Python/7_Binary%20Search/BinarySearch.py)

**Bonus: Si chiama Ricerca Binaria perch√© divide continuamente l‚Äôarray in due parti, con il risultato della parte sinistra o destra.**

## Depth-First Search (Ricerca Approfondita)

Questo algoritmo di ricerca √® appositamente progettato per grafici (graphs) e alberi (trees). Un grafico √® un insieme di nodi collegati. Ogni nodo √® chiamato vertice (vertex) e la connessione tra due di essi √® chiamata bordo (edge). Mentre un albero (trees) √® un grafico aciclico cio√® un grafico non orientato, in cui due vertici sono collegati esattamente da un percorso.

La Ricerca Approfondita (DFS) √® un algoritmo di ricerca ricorsiva che utilizza il **backtracking**, cio√®, inizia dalla radice dell‚Äôalbero (nodo arbitrario nel caso del grafico) ed esplora il pi√π possibile fino a quando tutti i nodi vengono visitati. **La parola backtrack significa che quando ci si sposta in avanti e non ci sono pi√π nodi lungo il percorso, ci si sposta indietro sullo stesso percorso per trovare nodi da attraversare.**

Esempio: eseguiamo l‚Äôalgoritmo sul grafico in basso e vediamo in quale ordine verranno visitati i nodi e inoltre supponiamo che l‚Äôalgoritmo scelga i bordi a sinistra prima di quelli a destra:

![](images/4.jpg)

1) Visitare il nodo A. Continuare sul nodo sinistro.
2) Visitare il nodo B. Continuare sul nodo sinistro.
3) Visitare il nodo D. Non ci sono pi√π nodi disponibili nel percorso, torna al nodo B.
4) Visitare il nodo F. Continuare con il nodo disponibile nel percorso.
5) Visitare il nodo E. Il nodo A √® gi√† stato visitato.
6) Tornare al nodo A. Visitare il nodo C. Continuare con il nodo disponibile nel percorso.
7) Visitare il nodo G. Tutti i nodi sono stati visitati

Quindi, ricordando i nodi visitati l‚Äôordine sar√†: A, B, D, F, E, C, G.

Senza ricordare i nodi visitati: A, B, D, F, E (il percorso si ripeter√† di nuovo) e non raggiuger√† mai i nodi C e G.

Se rimuoviamo il bordo tra i nodi F ed E, otterremo un albero (trees) e l‚Äôordine dei nodi visitati sar√†: A, B, D, F, C, G, E.

Implementazione: [C++](https://github.com/AntonioBerna/Algoritmi/blob/master/C%2B%2B/8_Depth-First%20Search/Depth-Fisrt%20Search.cpp), [Python](https://github.com/AntonioBerna/Algoritmi/blob/master/Python/8_Depth-First%20Search/Depth-Fisrt%20Search.py)

**Bonus: Il DFS viene solitamente implementato utilizzando uno Stack (cio√® una Struttura Dati) o un array / matrice di adiacenza.**

## Binary Search Tree

Binary Search Tree (BST) √® un albero binario speciale in cui la chiave in ciascun nodo deve essere maggiore o uguale a qualsiasi chiave memorizzata nella sottostruttura sinistra e minore o uguale a qualsiasi chiave memorizzata nella sottostruttura destra.

Per esempio:

![](images/3.png)

I BST consentono una rapida ricerca (aggiunta e rimozione di elementi) e possono essere utilizzati per implementare array dinamici o tabelle di ricerca che consentono di trovare un elemento tramite la sua chiave (ad esempio, trovare il numero di telefono di una persona con il cognome).

Le operazioni di ricerca, inserimento ed eliminazione sono fondamentali nei BST. Poich√© le chiavi sono memorizzate in un ordine particolare, il principio della ricerca binaria pu√≤ essere utilizzato per la ricerca. Partiamo dalla radice e confrontiamo la chiave. Se la radice ha la chiave, viene restituita come risultato. Se la chiave √® maggiore della radice, la sottostruttura destra viene controllata in modo ricorsivo. Se la chiave √® inferiore alla radice, la sottostruttura sinistra viene controllata in modo ricorsivo. La ricorsione continua fino a quando non viene trovata la chiave. Le operazioni di inserimento ed eliminazione sono molto simili, poich√© entrambe utilizzano la stessa logica di ricerca per individuare il nodo necessario.

Implementazione: [C++](https://github.com/AntonioBerna/Algoritmi/blob/master/C%2B%2B/9_Binary%20Search%20Tree/Binary%20Search%20Tree.cpp), [Python](https://github.com/AntonioBerna/Algoritmi/blob/master/Python/9_Binary%20Search%20Tree/Binary%20Search%20Tree.py)

**Bonus: Gli Alberi di Ricerca Binaria sono utilizzati in molte applicazioni di ricerca.**

# Time Complexity

In Informatica, la complessit√† temporale misura o stima il tempo impiegato per l‚Äôesecuzione di un algoritmo e viene stimata contando il numero di operazioni elementari eseguite dall‚Äôalgoritmo, supponendo che un‚Äôoperazione elementare richieda una quantit√† fissa di tempo per essere eseguita. Poich√© il tempo di esecuzione di un algoritmo pu√≤ variare con input diversi della stessa dimensione, si considera comunemente la complessit√† temporale peggiore espressa usando la notazione Big-O, che √® il tempo massimo impiegato sugli input di una data dimensione. Ad esempio, un algoritmo con complessit√† temporale ùëÇ(n) √® un algoritmo temporale lineare.

**√à comune escludere costanti e coefficienti di ordine inferiore che non hanno un impatto cos√¨ grande sulla complessit√† del problema. Ad esempio: ùëÇ(2ùëõ) e ùëÇ(ùëõ+5) sono uguali a ùëÇ(n).**

## Complessit√† Temporali Comuni

ùëÇ(1) ‚Üí ùëáùëíùëöùëùùëú ùê∂ùëúùë†ùë°ùëéùëõùë°ùëí: dato un input di dimensione n, √® sufficiente un solo passaggio per eseguire l‚Äôalgoritmo.

# Per maggiori informazioni

Mini Corso Base Algoritmi di Ordinamento [Youtube]: https://www.youtube.com/watch?v=KhsCEVO1zhk&t=23s

Created By Antonio Bernardini Copyright¬© 2020
